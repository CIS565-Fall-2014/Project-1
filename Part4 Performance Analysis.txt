Part 4:

NOTE: In Part 1 I decreased the number of nbodies to 700 because the original number given (5000) crashed my graphics card.

1) How does changing the tile and block sizes change performance? Why?
Increasing the number of tile/block sizes would improve performance/decrease performance time, because the more blocks there are, the more threads there are available to work on planet acceleration, velocity, and position calculations. Since threads can perform calculations concurrently, the more threads there are the more planet calculations we can perform at the same time. Having more planet calculations completed at the same time means that it will take less time to calculate the new planet positions at each time step, making the simulation run faster. Also, in the case where all threads from one block are in use, thereby stalling that block from completing new calculations, another block can continue calculations until it stalls, and this cycle would continue until you run out of blocks. However if you have enough blocks, then when the final block stalls, the first block will have completed its calculations and be able to undertake more calculations, allowing for quicker time steps.

2) How does changing the number of planets change performance? Why?
Increasing the number of planets decreases performance/increases performance time because the more planets, the more kernels/threads that are necessary to conduct the calculations for each planet. Since we only have so many blocks of so many threads, we can only perform a finite amount of calculations at once. Thus, when we add more planets and have more planets than threads it will take longer to complete all calculations because we need to wait until more threads become available, and therefore each time step will take longer to complete.

3) Without running experiments, how would you expect the serial and GPU versions of matrix_math to compare? Why?
Without running experiments, I would expect the GPU version of matrix_math to run far faster than the serial version. Matrix math is calculated based upon the rows of one matrix and the columns of another and therefore the calculations of row-column pairs can be conducted independently of one another. The GPU version of matrix math can run all N sets of calculations for the row-column pairs at the same time, since it can have multiple threads running calculations concurrently. Alternately, in the CPU version of matrix math each calculation for each final value in the resulting matrix must be conducted one at a time, since the calculations are being completed on a single thread. Thus, if it takes T time to complete matrix math calculations with the CPU, it will take 1/T time to complete the same calculations with the CPU.